{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d5407f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c2840c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets = tfds.load(name=\"imdb_reviews\", as_supervised=True)\n",
    "train_set, test_set = datasets[\"train\"], datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a7ac3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for review, label in train_set.take(1):\n",
    "    print(review)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8fcdd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['train', 'test', 'unsupervised'])\n"
     ]
    }
   ],
   "source": [
    "print(datasets.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89df098b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(25000, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(train_set.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33885026",
   "metadata": {},
   "source": [
    "*Exercise: Split the test set into a validation set (15,000) and a test set (10,000).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "991313b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_set = train_set.shuffle(buffer_size=25000, seed=42)\n",
    "train_set = train_set.batch(batch_size).prefetch(1)\n",
    "valid_set = test_set.take(15000).batch(batch_size).prefetch(1)\n",
    "test_set = test_set.skip(15000).batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b771a0e1",
   "metadata": {},
   "source": [
    "*Exercise: Create a binary classification model, using a `TextVectorization` layer to preprocess each review.*\n",
    "We will create a `TextVectorization` layer and adapt it to the training set. Let's use TF-IDF for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "403ee158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "max_tokens = 1000\n",
    "sample_reviews = train_set.map(lambda review, label: review)\n",
    "text_vectorization = TextVectorization(max_tokens=max_tokens, \n",
    "                                       output_mode = \"tf_idf\")\n",
    "text_vectorization.adapt(sample_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6158ab41",
   "metadata": {},
   "source": [
    "Good! Now let's take a look at the first 10 words in the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b55da277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[UNK]', 'the', 'and', 'a', 'of', 'to', 'is', 'in', 'it', 'i']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_vectorization.get_vocabulary()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90486acf",
   "metadata": {},
   "source": [
    "These are the most common words in the reviews.\n",
    "\n",
    "We are ready to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17c57a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\wengl\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 5s 5ms/step - loss: 0.4414 - accuracy: 0.8206 - val_loss: 0.3734 - val_accuracy: 0.8444\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3744 - accuracy: 0.8519 - val_loss: 0.4208 - val_accuracy: 0.8291\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.3305 - accuracy: 0.8657 - val_loss: 0.3675 - val_accuracy: 0.8521\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2762 - accuracy: 0.8870 - val_loss: 0.3626 - val_accuracy: 0.8514\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.2182 - accuracy: 0.9130 - val_loss: 0.4814 - val_accuracy: 0.8307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1aa00dbe790>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization, \n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd3c36",
   "metadata": {},
   "source": [
    "We get about 84.4% accuracy on the validation set after just the first epoch, but after that the model makes no significant progress. We will do better in Chapter 16. For now the point is just to perform efficient preprocessing using `tf.data` and Keras preprocessing layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83033c2",
   "metadata": {},
   "source": [
    "*Exercise: Add an `Embedding` layer and compute the mean embedding for each review, multiplied by the square root of the number of words (see Chapter 16). This rescaled mean embedding can then be passed to the rest of your model.*\n",
    "\n",
    "To compute the mean embedding for each review, and multiply it by the square root of the number of words in that review, we will need a little function. For each sentence, this function needs to compute $M \\times \\sqrt{N}$\n",
    ", where $M$ is the mean of all the word embeddings in the sentence (excluding padding tokens), and $N$ is the number of words in the sentence (also excluding padding tokens). We can rewrite $M$ as $\\dfrac{S}{N}$, where $S$ is the sum of all word embeddings (it does not matter whether or not we include the padding tokens in this sum, since their representation is a zero vector). So the function must return $M \\times \\sqrt{N} = \\dfrac{S}{N} \\times \\sqrt{N} = \\dfrac{S}{\\sqrt{N}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62d10dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[3 2 0]\n",
      " [1 0 0]], shape=(2, 3), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[2]\n",
      " [1]], shape=(2, 1), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[3.535534 , 4.9497476, 2.1213205],\n",
       "       [6.       , 0.       , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    print(not_pad)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)\n",
    "    print(n_words)\n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "\n",
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "compute_mean_embedding(another_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64e816",
   "metadata": {},
   "source": [
    "Let's check that this is correct. The first review contains 2 words (the last token is a zero vector, which represents the `<pad>` token). Let's compute the mean embedding for these 2 words, and multiply the result by the square root of 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a4ef6572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[3.535534 , 4.9497476, 2.1213202]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[0:1, :2], axis=1) * tf.sqrt(2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f5bb3",
   "metadata": {},
   "source": [
    "Looks good! Now let's check the second review, which contains just one word (we ignore the two padding tokens):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3f9d5387",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[6., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_mean(another_example[1:2, :1], axis=1) * tf.sqrt(1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f381381",
   "metadata": {},
   "source": [
    "Perfect. Now we're ready to train our final model. It's the same as before, except we replaced TF-IDF with ordinal encoding (`output_mode=\"int\"`) followed by an Embedding layer, followed by a Lambda layer that calls the compute_mean_embedding layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58dcedc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lambda/count_nonzero/Sum:0\", shape=(None, None), dtype=int64)\n",
      "Tensor(\"lambda/count_nonzero_1/Sum:0\", shape=(None, 1), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, Lambda, Dense\n",
    "\n",
    "embedding_size = 20\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "text_vectorization = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_tokens, output_mode=\"int\")\n",
    "text_vectorization.adapt(sample_reviews)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    text_vectorization,\n",
    "    Embedding(input_dim=max_tokens, output_dim=embedding_size, mask_zero=True),\n",
    "    Lambda(compute_mean_embedding),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "db9c3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "Tensor(\"sequential_2/lambda/count_nonzero/Sum:0\", shape=(None, None), dtype=int64)\n",
      "Tensor(\"sequential_2/lambda/count_nonzero_1/Sum:0\", shape=(None, 1), dtype=int64)\n",
      "Tensor(\"sequential_2/lambda/count_nonzero/Sum:0\", shape=(None, None), dtype=int64)\n",
      "Tensor(\"sequential_2/lambda/count_nonzero_1/Sum:0\", shape=(None, 1), dtype=int64)\n",
      "774/782 [============================>.] - ETA: 0s - loss: 0.3067 - accuracy: 0.8707Tensor(\"sequential_2/lambda/count_nonzero/Sum:0\", shape=(None, None), dtype=int64)\n",
      "Tensor(\"sequential_2/lambda/count_nonzero_1/Sum:0\", shape=(None, 1), dtype=int64)\n",
      "782/782 [==============================] - 9s 8ms/step - loss: 0.3064 - accuracy: 0.8708 - val_loss: 0.3193 - val_accuracy: 0.8614\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.3022 - accuracy: 0.8713 - val_loss: 0.3280 - val_accuracy: 0.8548\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.2995 - accuracy: 0.8704 - val_loss: 0.3255 - val_accuracy: 0.8611\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 6s 8ms/step - loss: 0.2963 - accuracy: 0.8720 - val_loss: 0.3209 - val_accuracy: 0.8603\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 6s 7ms/step - loss: 0.2925 - accuracy: 0.8734 - val_loss: 0.3539 - val_accuracy: 0.8438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1aa033a5b90>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b705ed11",
   "metadata": {},
   "source": [
    "The model is just marginally better using embeddings (but we will do better in Chapter 16). The pipeline looks fast enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
